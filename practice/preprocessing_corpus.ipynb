{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 말뭉치 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say goodbye and i say hello .\n"
     ]
    }
   ],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "text = text.lower()\n",
    "text = text.replace('.',' .')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {} # 딕셔너리\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word in word_to_id:\n",
    "        continue\n",
    "    new_id = len(word_to_id)\n",
    "    word_to_id[word] = new_id;\n",
    "    id_to_word[new_id] = word;\n",
    "    \n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [word_to_id[w] for w in words] # 문장에 담긴 단어들을 id로 표현\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 말뭉치 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text, index=0):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.',' .')\n",
    "    words = text.split()\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word in word_to_id:\n",
    "            continue\n",
    "        \n",
    "        new_id = len(word_to_id)+index\n",
    "        word_to_id[word] = new_id;\n",
    "        id_to_word[new_id] = word;\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 파일을 불러와 말뭉치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"sample_text.txt\", \"r\") # crawling by beautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in_line=\"\"\n",
    "for t in text:\n",
    "    text_in_line += t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.[3]\\n\\nThe parallel task in human and animal psychology is often referred to as concept learning.\\n\\nTo solve a given problem of supervised learning, one has to perform the following steps:\\n\\nA wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\\n\\nThere are four major issues to consider in supervised learning:\\n\\nA first issue is the tradeoff between bias and variance.[4] Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file, word_to_id_file, id_to_word_file = preprocess(text_in_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 파일 말뭉치 처리 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'supervised',\n",
       " 1: 'learning',\n",
       " 2: '(sl)',\n",
       " 3: 'is',\n",
       " 4: 'the',\n",
       " 5: 'machine',\n",
       " 6: 'task',\n",
       " 7: 'of',\n",
       " 8: 'a',\n",
       " 9: 'function',\n",
       " 10: 'that',\n",
       " 11: 'maps',\n",
       " 12: 'an',\n",
       " 13: 'input',\n",
       " 14: 'to',\n",
       " 15: 'output',\n",
       " 16: 'based',\n",
       " 17: 'on',\n",
       " 18: 'example',\n",
       " 19: 'input-output',\n",
       " 20: 'pairs',\n",
       " 21: '.[1]',\n",
       " 22: 'it',\n",
       " 23: 'infers',\n",
       " 24: 'from',\n",
       " 25: 'labeled',\n",
       " 26: 'training',\n",
       " 27: 'data',\n",
       " 28: 'consisting',\n",
       " 29: 'set',\n",
       " 30: 'examples',\n",
       " 31: '.[2]',\n",
       " 32: 'in',\n",
       " 33: 'learning,',\n",
       " 34: 'each',\n",
       " 35: 'pair',\n",
       " 36: 'object',\n",
       " 37: '(typically',\n",
       " 38: 'vector)',\n",
       " 39: 'and',\n",
       " 40: 'desired',\n",
       " 41: 'value',\n",
       " 42: '(also',\n",
       " 43: 'called',\n",
       " 44: 'supervisory',\n",
       " 45: 'signal)',\n",
       " 46: '.',\n",
       " 47: 'algorithm',\n",
       " 48: 'analyzes',\n",
       " 49: 'produces',\n",
       " 50: 'inferred',\n",
       " 51: 'function,',\n",
       " 52: 'which',\n",
       " 53: 'can',\n",
       " 54: 'be',\n",
       " 55: 'used',\n",
       " 56: 'for',\n",
       " 57: 'mapping',\n",
       " 58: 'new',\n",
       " 59: 'optimal',\n",
       " 60: 'scenario',\n",
       " 61: 'will',\n",
       " 62: 'allow',\n",
       " 63: 'correctly',\n",
       " 64: 'determine',\n",
       " 65: 'class',\n",
       " 66: 'labels',\n",
       " 67: 'unseen',\n",
       " 68: 'instances',\n",
       " 69: 'this',\n",
       " 70: 'requires',\n",
       " 71: 'generalize',\n",
       " 72: 'situations',\n",
       " 73: '\"reasonable\"',\n",
       " 74: 'way',\n",
       " 75: '(see',\n",
       " 76: 'inductive',\n",
       " 77: 'bias)',\n",
       " 78: 'statistical',\n",
       " 79: 'quality',\n",
       " 80: 'measured',\n",
       " 81: 'through',\n",
       " 82: 'so-called',\n",
       " 83: 'generalization',\n",
       " 84: 'error',\n",
       " 85: '.[3]',\n",
       " 86: 'parallel',\n",
       " 87: 'human',\n",
       " 88: 'animal',\n",
       " 89: 'psychology',\n",
       " 90: 'often',\n",
       " 91: 'referred',\n",
       " 92: 'as',\n",
       " 93: 'concept',\n",
       " 94: 'solve',\n",
       " 95: 'given',\n",
       " 96: 'problem',\n",
       " 97: 'one',\n",
       " 98: 'has',\n",
       " 99: 'perform',\n",
       " 100: 'following',\n",
       " 101: 'steps:',\n",
       " 102: 'wide',\n",
       " 103: 'range',\n",
       " 104: 'algorithms',\n",
       " 105: 'are',\n",
       " 106: 'available,',\n",
       " 107: 'with',\n",
       " 108: 'its',\n",
       " 109: 'strengths',\n",
       " 110: 'weaknesses',\n",
       " 111: 'there',\n",
       " 112: 'no',\n",
       " 113: 'single',\n",
       " 114: 'works',\n",
       " 115: 'best',\n",
       " 116: 'all',\n",
       " 117: 'problems',\n",
       " 118: 'free',\n",
       " 119: 'lunch',\n",
       " 120: 'theorem)',\n",
       " 121: 'four',\n",
       " 122: 'major',\n",
       " 123: 'issues',\n",
       " 124: 'consider',\n",
       " 125: 'learning:',\n",
       " 126: 'first',\n",
       " 127: 'issue',\n",
       " 128: 'tradeoff',\n",
       " 129: 'between',\n",
       " 130: 'bias',\n",
       " 131: 'variance',\n",
       " 132: '.[4]',\n",
       " 133: 'imagine',\n",
       " 134: 'we',\n",
       " 135: 'have',\n",
       " 136: 'available',\n",
       " 137: 'several',\n",
       " 138: 'different,',\n",
       " 139: 'but',\n",
       " 140: 'equally',\n",
       " 141: 'good,',\n",
       " 142: 'sets',\n",
       " 143: 'biased',\n",
       " 144: 'particular'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'supervised': 0,\n",
       " 'learning': 1,\n",
       " '(sl)': 2,\n",
       " 'is': 3,\n",
       " 'the': 4,\n",
       " 'machine': 5,\n",
       " 'task': 6,\n",
       " 'of': 7,\n",
       " 'a': 8,\n",
       " 'function': 9,\n",
       " 'that': 10,\n",
       " 'maps': 11,\n",
       " 'an': 12,\n",
       " 'input': 13,\n",
       " 'to': 14,\n",
       " 'output': 15,\n",
       " 'based': 16,\n",
       " 'on': 17,\n",
       " 'example': 18,\n",
       " 'input-output': 19,\n",
       " 'pairs': 20,\n",
       " '.[1]': 21,\n",
       " 'it': 22,\n",
       " 'infers': 23,\n",
       " 'from': 24,\n",
       " 'labeled': 25,\n",
       " 'training': 26,\n",
       " 'data': 27,\n",
       " 'consisting': 28,\n",
       " 'set': 29,\n",
       " 'examples': 30,\n",
       " '.[2]': 31,\n",
       " 'in': 32,\n",
       " 'learning,': 33,\n",
       " 'each': 34,\n",
       " 'pair': 35,\n",
       " 'object': 36,\n",
       " '(typically': 37,\n",
       " 'vector)': 38,\n",
       " 'and': 39,\n",
       " 'desired': 40,\n",
       " 'value': 41,\n",
       " '(also': 42,\n",
       " 'called': 43,\n",
       " 'supervisory': 44,\n",
       " 'signal)': 45,\n",
       " '.': 46,\n",
       " 'algorithm': 47,\n",
       " 'analyzes': 48,\n",
       " 'produces': 49,\n",
       " 'inferred': 50,\n",
       " 'function,': 51,\n",
       " 'which': 52,\n",
       " 'can': 53,\n",
       " 'be': 54,\n",
       " 'used': 55,\n",
       " 'for': 56,\n",
       " 'mapping': 57,\n",
       " 'new': 58,\n",
       " 'optimal': 59,\n",
       " 'scenario': 60,\n",
       " 'will': 61,\n",
       " 'allow': 62,\n",
       " 'correctly': 63,\n",
       " 'determine': 64,\n",
       " 'class': 65,\n",
       " 'labels': 66,\n",
       " 'unseen': 67,\n",
       " 'instances': 68,\n",
       " 'this': 69,\n",
       " 'requires': 70,\n",
       " 'generalize': 71,\n",
       " 'situations': 72,\n",
       " '\"reasonable\"': 73,\n",
       " 'way': 74,\n",
       " '(see': 75,\n",
       " 'inductive': 76,\n",
       " 'bias)': 77,\n",
       " 'statistical': 78,\n",
       " 'quality': 79,\n",
       " 'measured': 80,\n",
       " 'through': 81,\n",
       " 'so-called': 82,\n",
       " 'generalization': 83,\n",
       " 'error': 84,\n",
       " '.[3]': 85,\n",
       " 'parallel': 86,\n",
       " 'human': 87,\n",
       " 'animal': 88,\n",
       " 'psychology': 89,\n",
       " 'often': 90,\n",
       " 'referred': 91,\n",
       " 'as': 92,\n",
       " 'concept': 93,\n",
       " 'solve': 94,\n",
       " 'given': 95,\n",
       " 'problem': 96,\n",
       " 'one': 97,\n",
       " 'has': 98,\n",
       " 'perform': 99,\n",
       " 'following': 100,\n",
       " 'steps:': 101,\n",
       " 'wide': 102,\n",
       " 'range': 103,\n",
       " 'algorithms': 104,\n",
       " 'are': 105,\n",
       " 'available,': 106,\n",
       " 'with': 107,\n",
       " 'its': 108,\n",
       " 'strengths': 109,\n",
       " 'weaknesses': 110,\n",
       " 'there': 111,\n",
       " 'no': 112,\n",
       " 'single': 113,\n",
       " 'works': 114,\n",
       " 'best': 115,\n",
       " 'all': 116,\n",
       " 'problems': 117,\n",
       " 'free': 118,\n",
       " 'lunch': 119,\n",
       " 'theorem)': 120,\n",
       " 'four': 121,\n",
       " 'major': 122,\n",
       " 'issues': 123,\n",
       " 'consider': 124,\n",
       " 'learning:': 125,\n",
       " 'first': 126,\n",
       " 'issue': 127,\n",
       " 'tradeoff': 128,\n",
       " 'between': 129,\n",
       " 'bias': 130,\n",
       " 'variance': 131,\n",
       " '.[4]': 132,\n",
       " 'imagine': 133,\n",
       " 'we': 134,\n",
       " 'have': 135,\n",
       " 'available': 136,\n",
       " 'several': 137,\n",
       " 'different,': 138,\n",
       " 'but': 139,\n",
       " 'equally': 140,\n",
       " 'good,': 141,\n",
       " 'sets': 142,\n",
       " 'biased': 143,\n",
       " 'particular': 144}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리가 보다 필요함을 알수있는데, 온점과 소괄호, 대괄호, 쌍따옴표가 눈에 띄인다. 문서의 종류에 따라 다르겠지만 숫자도 언어처리에는 불필요 하지 않을까 생각이 된다. \n",
    "\n",
    "추가적으로 처리를 하자면 상대적으로 의미를 가지지 않는 단어들이 있는데 be동사나 전치사 등이 그렇다. 이런 경우에도 삭제해주어도 문서의 뜻을 해치지 않을 것이라고 생각된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = corpus_file.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   1,   6,   7,   1,   8,   9,  10,\n",
       "        11,  12,  13,  14,  12,  15,  16,  17,  18,  19,  20,  21,  22,\n",
       "        23,   8,   9,  24,  25,  26,  27,  28,   7,   8,  29,   7,  26,\n",
       "        30,  31,  32,   0,  33,  34,  18,   3,   8,  35,  28,   7,  12,\n",
       "        13,  36,  37,   8,  38,  39,   8,  40,  15,  41,  42,  43,   4,\n",
       "        44,  45,  46,   8,   0,   1,  47,  48,   4,  26,  27,  39,  49,\n",
       "        12,  50,  51,  52,  53,  54,  55,  56,  57,  58,  30,  46,  12,\n",
       "        59,  60,  61,  62,  56,   4,  47,  14,  63,  64,   4,  65,  66,\n",
       "        56,  67,  68,  46,  69,  70,   4,   1,  47,  14,  71,  24,   4,\n",
       "        26,  27,  14,  67,  72,  32,   8,  73,  74,  75,  76,  77,  46,\n",
       "        69,  78,  79,   7,  12,  47,   3,  80,  81,   4,  82,  83,  84,\n",
       "        85,   4,  86,   6,  32,  87,  39,  88,  89,   3,  90,  91,  14,\n",
       "        92,  93,   1,  46,  14,  94,   8,  95,  96,   7,   0,  33,  97,\n",
       "        98,  14,  99,   4, 100, 101,   8, 102, 103,   7,   0,   1, 104,\n",
       "       105, 106,  34, 107, 108, 109,  39, 110,  46, 111,   3, 112, 113,\n",
       "         1,  47,  10, 114, 115,  17, 116,   0,   1, 117,  75,   4, 112,\n",
       "       118, 119, 120,  46, 111, 105, 121, 122, 123,  14, 124,  32,   0,\n",
       "       125,   8, 126, 127,   3,   4, 128, 129, 130,  39, 131, 132, 133,\n",
       "        10, 134, 135, 136, 137, 138, 139, 140, 141,  26,  27, 142,  46,\n",
       "         8,   1,  47,   3, 143,  56,   8, 144,  13])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동시 발생 행렬 co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 단어를 기준으로 주변단어에 어떤 단어가 등장하는 지(분포가설)를 이용하여 동시발생 행렬을 만들 수 있다.(책에서는 통계 기반 기법이라고 지칭함.) 이를 이용하면 단어를 벡터로 표현 할수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you say goodbey and i say hello .의 동시발생 행렬\n",
    "\n",
    "C = np.array([[0,1,0,0,0,0,0],\n",
    "              [1,0,1,0,1,1,0],\n",
    "              [0,1,0,1,0,0,0],\n",
    "              [0,0,1,0,1,0,0],\n",
    "              [0,1,0,1,0,0,0],\n",
    "              [0,1,0,0,0,0,1],\n",
    "              [0,0,0,0,0,1,0]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "text = text.lower()\n",
    "text = text.replace('.',' .')\n",
    "words = text.split()\n",
    "\n",
    "word_to_id = {} # 딕셔너리\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word in word_to_id:\n",
    "        continue\n",
    "    new_id = len(word_to_id)\n",
    "    word_to_id[word] = new_id;\n",
    "    id_to_word[new_id] = word;\n",
    "    \n",
    "corpus = [word_to_id[w] for w in words] # 문장에 담긴 단어들을 id로 표현\n",
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[word_to_id['you']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size,vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size+1): # 현재 단어를 기준으로 좌우 얼마나 살펴볼지\n",
    "            left_idx = idx - i;\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx>=0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id][left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id][right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 직접 만든 C와 같은 결과가 나오는 것을 볼수 있음.\n",
    "create_co_matrix(corpus, len(word_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 4, 0, ..., 0, 0, 0],\n",
       "       [4, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트 파일을 통해 생성한 말뭉치과 딕셔너리를 이용하여 동시발생 행렬을 만들어 봄\n",
    "create_co_matrix(corpus_file, len(word_to_id_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터간 유사도 (cosine similarity)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "동시발생 행렬을 이용하여 각 단어들을 벡터로 표현할 수 있게 되었다. 이게 벡터간의 유사도를 활용하여 단어들이 서로 얼마나 연관되어 있는지를 계산 해볼 것이다. 두 벡터의 각도에 코사인을 입힌 값을 코사인 유사도라고 하며, 내적과 L2 norm을 이용하여 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y는 numpy array라고 가정\n",
    "# 0으로 나누는 경우를 방지하기 위해 아주 작은 값을 더해준다\n",
    "\n",
    "def cos_similarity(x,y, eps = 1e-8): \n",
    "    nx = x/ np.sqrt(np.sum(x**2) + eps) # x의 정규화 \n",
    "    ny = y/ np.sqrt(np.sum(y**2) + eps) # y의 정규화\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067758832467\n"
     ]
    }
   ],
   "source": [
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "print(cos_similarity(c0,c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similarity(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    # 검색어 꺼내기\n",
    "    if query not in word_to_id:\n",
    "        print(\"%s를 찾을 수 없습니다.\"%query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query]' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    # 코사인 유사도를 기준으로 내림차순 ㄱ출력\n",
    "    count=0\n",
    "    for i in(-1 * similarity).argsort(): # 오름차순 정렬& 배열의 인덱스를 리턴\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "            \n",
    "        print(\" %s: %s\" %(id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count+=1\n",
    "        if count>= top:\n",
    "            return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query]you\n",
      " goodbye: 0.7071067758832467\n",
      " i: 0.7071067758832467\n",
      " hello: 0.7071067758832467\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "# you say goodbye and i say hello 문장을 분석한 결과\n",
    "\n",
    "most_similarity('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query]learning\n",
      " learning:: 0.6255432405393724\n",
      " analyzes: 0.5212860337828102\n",
      " in: 0.5128776442199862\n",
      " learning,: 0.481543411889449\n",
      " is: 0.45178123028239386\n",
      " all: 0.4170288270262482\n",
      " of: 0.36265926023759854\n",
      " to: 0.3127716209647342\n",
      " set: 0.3127716202696862\n",
      " an: 0.2580234231623917\n"
     ]
    }
   ],
   "source": [
    "# widipedia에서 supervised learning에 단 문서 일부를 사용한 결과\n",
    "\n",
    "C = create_co_matrix(corpus_file, len(word_to_id_file))\n",
    "most_similarity('learning', word_to_id_file, id_to_word_file, C, top=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과를 보면 전처리가 더 필요함을 알수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
