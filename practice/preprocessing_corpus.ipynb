{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 말뭉치 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you say goodbye and i say hello .\n"
     ]
    }
   ],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "text = text.lower()\n",
    "text = text.replace('.',' .')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {} # 딕셔너리\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word in word_to_id:\n",
    "        continue\n",
    "    new_id = len(word_to_id)\n",
    "    word_to_id[word] = new_id;\n",
    "    id_to_word[new_id] = word;\n",
    "    \n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [word_to_id[w] for w in words] # 문장에 담긴 단어들을 id로 표현\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 말뭉치 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.',' .')\n",
    "    words = text.split()\n",
    "    \n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word in word_to_id:\n",
    "            continue\n",
    "        \n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id;\n",
    "        id_to_word[new_id] = word;\n",
    "    \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 파일을 불러와 말뭉치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"sample_text.txt\", \"r\") # crawling by beautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id_file = {} # 딕셔너리\n",
    "id_to_word_file = {}\n",
    "corpus_file = np.array([])\n",
    "for t in text:\n",
    "    temp, w_to_id, id_to_w = preprocess(t)\n",
    "    corpus_file = np.append(corpus_file,temp)\n",
    "    word_to_id_file.update(w_to_id)\n",
    "    id_to_word_file.update(id_to_w)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 파일 말뭉치 처리 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a',\n",
       " 1: 'first',\n",
       " 2: 'issue',\n",
       " 3: 'is',\n",
       " 4: 'the',\n",
       " 5: 'tradeoff',\n",
       " 6: 'between',\n",
       " 7: 'bias',\n",
       " 8: 'and',\n",
       " 9: 'variance',\n",
       " 10: '.[4]',\n",
       " 11: 'imagine',\n",
       " 12: 'that',\n",
       " 13: 'we',\n",
       " 14: 'have',\n",
       " 15: 'available',\n",
       " 16: 'several',\n",
       " 17: 'different,',\n",
       " 18: 'but',\n",
       " 19: 'equally',\n",
       " 20: 'good,',\n",
       " 21: 'training',\n",
       " 22: 'data',\n",
       " 23: 'sets',\n",
       " 24: '.',\n",
       " 25: 'learning',\n",
       " 26: 'algorithm',\n",
       " 27: 'biased',\n",
       " 28: 'for',\n",
       " 29: 'particular',\n",
       " 30: 'input',\n",
       " 31: 'theorem)',\n",
       " 32: 'in',\n",
       " 33: 'learning,',\n",
       " 34: 'each',\n",
       " 35: 'pair',\n",
       " 36: 'object',\n",
       " 37: '(typically',\n",
       " 38: 'vector)',\n",
       " 39: 'and',\n",
       " 40: 'desired',\n",
       " 41: 'value',\n",
       " 42: '(also',\n",
       " 43: 'called',\n",
       " 44: 'supervisory',\n",
       " 45: 'signal)',\n",
       " 46: '.',\n",
       " 47: 'algorithm',\n",
       " 48: 'analyzes',\n",
       " 49: 'produces',\n",
       " 50: 'inferred',\n",
       " 51: 'function,',\n",
       " 52: 'which',\n",
       " 53: 'can',\n",
       " 54: 'be',\n",
       " 55: 'used',\n",
       " 56: 'for',\n",
       " 57: 'mapping',\n",
       " 58: 'new',\n",
       " 59: 'optimal',\n",
       " 60: 'scenario',\n",
       " 61: 'will',\n",
       " 62: 'allow',\n",
       " 63: 'correctly',\n",
       " 64: 'determine',\n",
       " 65: 'class',\n",
       " 66: 'labels',\n",
       " 67: 'unseen',\n",
       " 68: 'instances',\n",
       " 69: 'this',\n",
       " 70: 'requires',\n",
       " 71: 'generalize',\n",
       " 72: 'situations',\n",
       " 73: '\"reasonable\"',\n",
       " 74: 'way',\n",
       " 75: '(see',\n",
       " 76: 'inductive',\n",
       " 77: 'bias)',\n",
       " 78: 'statistical',\n",
       " 79: 'quality',\n",
       " 80: 'measured',\n",
       " 81: 'through',\n",
       " 82: 'so-called',\n",
       " 83: 'generalization',\n",
       " 84: 'error',\n",
       " 85: '.[3]'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  1,  6,  7,  1,  8,  9, 10, 11, 12, 13, 14,\n",
       "       12, 15, 16, 17, 18, 19, 20, 21, 22, 23,  8,  9, 24, 25, 26, 27, 28,\n",
       "        7,  8, 29,  7, 26, 30, 31, 32,  0, 33, 34, 18,  3,  8, 35, 28,  7,\n",
       "       12, 13, 36, 37,  8, 38, 39,  8, 40, 15, 41, 42, 43,  4, 44, 45, 46,\n",
       "        8,  0,  1, 47, 48,  4, 26, 27, 39, 49, 12, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 30, 46, 12, 59, 60, 61, 62, 56,  4, 47, 14, 63, 64,  4,\n",
       "       65, 66, 56, 67, 68, 46, 69, 70,  4,  1, 47, 14, 71, 24,  4, 26, 27,\n",
       "       14, 67, 72, 32,  8, 73, 74, 75, 76, 77, 46, 69, 78, 79,  7, 12, 47,\n",
       "        3, 80, 81,  4, 82, 83, 84, 85,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "        9, 10, 11, 12, 13, 14, 15,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9,\n",
       "        0, 10, 11, 12, 13,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "       12, 13, 14, 15, 16, 17, 18, 19,  5, 20, 21, 22, 23, 24, 25,  4,  5,\n",
       "       26, 27, 28, 18, 29, 30, 31, 15,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "        9,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17, 18, 19, 20, 21, 22, 23, 24,  0, 25, 26,  3, 27, 28,  0, 29,\n",
       "       30])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동시 발생 행렬 co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you say goodbey and i say hello .의 동시발생 행렬\n",
    "\n",
    "C = np.array([[0,1,0,0,0,0,0],\n",
    "              [1,0,1,0,1,1,0],\n",
    "              [0,1,0,1,0,0,0],\n",
    "              [0,0,1,0,1,0,0],\n",
    "              [0,1,0,1,0,0,0],\n",
    "              [0,1,0,0,0,0,1],\n",
    "              [0,0,0,0,0,1,0]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "text = text.lower()\n",
    "text = text.replace('.',' .')\n",
    "words = text.split()\n",
    "\n",
    "word_to_id = {} # 딕셔너리\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word in word_to_id:\n",
    "        continue\n",
    "    new_id = len(word_to_id)\n",
    "    word_to_id[word] = new_id;\n",
    "    id_to_word[new_id] = word;\n",
    "    \n",
    "corpus = [word_to_id[w] for w in words] # 문장에 담긴 단어들을 id로 표현\n",
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[word_to_id['you']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size,vocab_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size+1): # 현재 단어를 기준으로 좌우 얼마나 살펴볼지\n",
    "            left_idx = idx - i;\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx>=0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id][left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id][right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 직접 만든 C와 같은 결과가 나오는 것을 볼수 있음.\n",
    "create_co_matrix(corpus, len(word_to_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
